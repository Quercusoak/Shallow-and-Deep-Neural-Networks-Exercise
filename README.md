# Shallow and Deep Neural Networks Exercise
Exrecise in training and optimization of neural networks for two tasks: classifying handwritten digits from the MNIST dataset and learning the XOR function.
It demonstrates the importance of hyperparameter tuning and the relationship between network architecture, data representation, and training parameters.

## MNIST Classification
The code trains a simple feedforward neural network with three layers to classify handwritten digits from the MNIST dataset.
The goal is to improve the network's accuracy by tuning hyperparameters like the loss function, batch size, and learning rate.

## XOR Function
The code trains a neural network to learn the XOR function with different input sizes.
The goals are to:
- Achieve the best accuracy by tuning hyperparameters like input representation, network architecture, activation functions, batch size, epochs, and learning rate.
- Study the effect of these hyperparameters on the network's performance.
- Interpret the representation in the hidden layers of the final network.
